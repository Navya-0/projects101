{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTRAVC3F4ksQ+gw4GMnaub",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Navya-0/projects101/blob/main/paragraph_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "05TOEzECr_nW",
        "outputId": "5c42b230-0efa-4fc6-e593-bf5fd7c77d03"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' we will be using hugging face that allows us to use gpt2 that is state of art nlp model, \\nusing this model we can generate a text paragraph as an output with input being a single line of text. \\n\\nwe will first set up our gpt2 model \\nthen we pass an input text which will then be encoded\\nafter encooding the tool will decode it and will produce an output text wj=hich can then be explored to generate a paragraph. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "''' we will be using hugging face that allows us to use gpt2 that is state of art nlp model,\n",
        "using this model we can generate a text paragraph as an output with input being a single line of text.\n",
        "\n",
        "we will first set up our gpt2 model\n",
        "then we pass an input text which will then be encoded\n",
        "after encooding the tool will decode it and will produce an output text wj=hich can then be explored to generate a paragraph. '''\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# installing hugging face and importing dependencies\n",
        "\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54w4aQ6GsD__",
        "outputId": "bb7f068d-06dd-47f3-93ee-3aef7261a7fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "''' here the tokenizer will first break the text into tokens and then convert those tokens into their numeric equivalent,\n",
        "after this the numbers are passed to the LMHhead model this model will then produce the output text in numberic form,\n",
        "this will then be taken by the tokenizer that will then transform these numeric output to their text equivalent.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "70uMbyGxsEDQ",
        "outputId": "659d6bb4-f75b-434c-84ac-486e71185b0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' here the tokenizer will first break the text into tokens and then convert those tokens into their numeric equivalent,\\nafter this the numbers are passed to the LMHhead model this model will then produce the output text in numberic form,\\nthis will then be taken by the tokenizer that will then transform these numeric output to their text equivalent.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large') # gpt2-large model allows us to generate and process large blocks of texts\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2-large', pad_token_id = tokenizer.eos_token_id) # pad_token is used to decide what token are we going to use for padding, here we are using tokenizer.eos_token_id"
      ],
      "metadata": {
        "id": "H8vAynUcsEGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sentence tokenization\n",
        "st = \"AI is a boon\"\n",
        "input_ids = tokenizer.encode(st, return_tensors =\"pt\")"
      ],
      "metadata": {
        "id": "Ak-TlqIksAgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids # these are the numeric form of identifier for each word in the sentence\n",
        "tokenizer.decode(input_ids[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aIrZVPonsAjh",
        "outputId": "056b4235-5151-4492-e14e-8ff855922219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AI is a boon'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generation of text blog\n",
        "output= model.generate(input_ids, max_length =600, num_beams=5, no_repeat_ngram_size=2, early_stopping= True) # beams find which next word to put\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssm3iWeLsAma",
        "outputId": "e436cfdd-c487-4317-8b1f-59b064c7958a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[20185,   318,   257, 38181,   284,   262,  2831,    11,   475,   340,\n",
              "           338,   407,   257,  3425, 44977,    13,   198,   198,     1,  1858,\n",
              "           338,   257,  1256,   286,   670,   326,  2476,   284,   307,  1760,\n",
              "           284,   787,  1654,   326,   428,  3037,   318,   973,   287,   257,\n",
              "           835,   326,   318,  3338,   290,  4050,    11,   290,   326,   340,\n",
              "          1595,   470,  1085,   284, 30261,  6948,   553,   339,   531,    13,\n",
              "           366,  1135,   761,   284,   466,   257,  1365,  1693,   286, 36267,\n",
              "           262,  1171,   546,   262,  7476,   290,   262,  4034,   286,   262,\n",
              "          3037,   526, 50256]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(output[0], skip_special_tokens = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "9asc1h76sApZ",
        "outputId": "f1cad87e-9d70-40dc-c852-631a6d7546b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AI is a boon to the industry, but it\\'s not a panacea.\\n\\n\"There\\'s a lot of work that needs to be done to make sure that this technology is used in a way that is safe and effective, and that it doesn\\'t lead to unintended consequences,\" he said. \"We need to do a better job of educating the public about the risks and the benefits of the technology.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = tokenizer.decode(output[0], skip_special_tokens= True)"
      ],
      "metadata": {
        "id": "CZvIOUdoavpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"mypet.txt\", \"w\") as f:\n",
        "  f.write(text)"
      ],
      "metadata": {
        "id": "FpG8wZvhavsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QAvWDdthavwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FtomFBotsAsu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}