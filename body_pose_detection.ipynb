{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Navya-0/projects101/blob/main/body_pose_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "D18n1GuQEcUo",
        "outputId": "522f02f0-af66-4cc9-84cc-e0e458260299"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In computer science jargon, a graph consists of Nodes connected by Edges.\\nInside the MediaPipe Graph, the nodes are called Calculators, and the edges are called Streams. \\nEvery stream carries a sequence of Packets that have ascending time stamps. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "'''MediaPipe is a Framework for building machine learning pipelines for processing time-series data like video, audio, etc. \n",
        "This cross-platform Framework works on Desktop/Server, Android, iOS, and embedded devices like Raspberry Pi and Jetson Nano.'''\n",
        "\n",
        "'''The MediaPipe perception pipeline is called a Graph. \n",
        "Let us take the example of the first solution, Hands.\n",
        "We feed a stream of images as input which comes out with hand landmarks rendered on the images.'''\n",
        "\n",
        "'''In computer science jargon, a graph consists of Nodes connected by Edges.\n",
        "Inside the MediaPipe Graph, the nodes are called Calculators, and the edges are called Streams. \n",
        "Every stream carries a sequence of Packets that have ascending time stamps. '''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import time\n",
        "import math as m\n",
        "!pip install mediapipe\n",
        "import mediapipe as mp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIQfp0aPE4ow",
        "outputId": "bf05a689-de4e-4ea0-eaaa-78d5ac0b25ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.9/dist-packages (0.9.1.0)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.9/dist-packages (from mediapipe) (4.7.0.72)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from mediapipe) (23.3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from mediapipe) (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from mediapipe) (3.7.1)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.9/dist-packages (from mediapipe) (3.19.6)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.9/dist-packages (from mediapipe) (22.2.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (5.12.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (8.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (23.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (4.39.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (3.0.9)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->mediapipe) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#distance =  sqrt{(x2 - x1)^2+(y2 - y1)^2}, \n",
        "'''The function findDistance helps us determine the offset distance between two points. \n",
        "It can be the hip points, the eyes, or the shoulder.\n",
        "These points have been selected as they are always more or less symmetric about the central axis of the human body. \n",
        "With this, we will incorporate the camera alignment feature in the script'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "NLkm5A0OE4sA",
        "outputId": "3bde87a5-6d12-4984-808c-385146fa8298"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The function findDistance helps us determine the offset distance between two points. \\nIt can be the hip points, the eyes, or the shoulder.\\nThese points have been selected as they are always more or less symmetric about the central axis of the human body. \\nWith this, we will incorporate the camera alignment feature in the script'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def findDistance(x1,y1,x2,y2):\n",
        "  dist= m.sqrt((x1-x2)**2,(y2-y1)**2)\n",
        "  return dist"
      ],
      "metadata": {
        "id": "zOZS0yjUE4vN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Function to Calculate the Body Posture Inclination**"
      ],
      "metadata": {
        "id": "XKCMrpZiPFaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def findAngle(x1,y1,x2,y2):\n",
        "  theta=(y2-y1)*(-y1)/(m.sqrt((x2-x1)**2,(y2-y1)**2)*y1)\n",
        "  degree= int(180/m.pi)*theta\n",
        "  return degree"
      ],
      "metadata": {
        "id": "JeH1OSFTE4yX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to Send Poor Body Posture **Alerts**"
      ],
      "metadata": {
        "id": "HfqOc0k6QaId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sendWarning(x):\n",
        "  https://script.google.com/macros/library/d/1NhUVKYt6j8gQRj7Z5hbxZkzarJGm21jnc_7srPy9mZDMRyJyHdS-ICVf/1\n",
        "  pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "9bLDIdmXE411",
        "outputId": "3b306344-7689-44ef-b0cd-1cae992820e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-7782c7e7eee8>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    https://script.google.com/macros/library/d/1NhUVKYt6j8gQRj7Z5hbxZkzarJGm21jnc_7srPy9mZDMRyJyHdS-ICVf/1\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Initializations**"
      ],
      "metadata": {
        "id": "ib9xY0HxFfPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "good_frames= 0\n",
        "bad_frames= 0\n",
        "\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "\n",
        "blue=(255,127,0)\n",
        "red=(50,50,255)\n",
        "green=(127,255,0)\n",
        "dark_blue=(127,20,0)\n",
        "light_green=(127,233,100)\n",
        "yellow=(0,255,255)\n",
        "pink=(255,0,255)\n",
        "\n",
        "# pose class initialise\n",
        "mp_pose = mp.solutions.pose\n",
        "pose= mp_pose.Pose()"
      ],
      "metadata": {
        "id": "ktYwdfsmE45H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Video Capture and Video Writer **Objects**"
      ],
      "metadata": {
        "id": "31KwioKsFp75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = 0\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "frame_size = (width,height)\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "video_output = cv2.VideoWriter('output.mp4', fourcc, fps, frame_size)"
      ],
      "metadata": {
        "id": "eL872pHxE48o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Body Posture Detection Main **Loop**"
      ],
      "metadata": {
        "id": "dK6hf7MkFxYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "success, image = cap.read()\n",
        "while not success:\n",
        "  print(\"Null.Frames\")\n",
        "  break\n",
        "\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "h,w = image.Shape[:2]\n",
        "\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        " \n",
        "keypoints = pose.process(image)\n",
        " \n",
        "image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "77IC7f5z_JoF",
        "outputId": "e9132fd1-29e8-4e26-b48c-7495fc3ab2f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Null.Frames\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-04a22c1c73e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCAP_PROP_FPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mShape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'Shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Acquire Body Posture Landmark Coordinates**"
      ],
      "metadata": {
        "id": "n41CO_hTF6Ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "norm_coordinate= pose.process(image).pose_landmark.landmark[MediaPipe.solutions.pose.PoseLandmark.<SPECIFIC_LANDMARK>].coordinate"
      ],
      "metadata": {
        "id": "LelxtO1V_JpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm = keypoints.pose_landmarks\n",
        "lmPose = mp_pose.PoseLandmark\n",
        "\n",
        "lshouldr_x =\n",
        "lshouldr_y =\n",
        "\n",
        "rshouldr_x =\n",
        "rshouldr_y =\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NxZaFXVxE5AH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XM9ACjj-E5Dl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}